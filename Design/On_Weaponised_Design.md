---
created: 2021-10-24T07:23:47 (UTC -07:00)
tags: []
source: https://shiba.computer/essay/on-weaponised-design
author: Shiba Computer
---

# On Weaponised Design ~ Shiba Computer

> ## Excerpt
> The lived experiences of digital platform users is at odds with how these systems are designed. Weaponised design – a process that allows for harm of users within the defined bounds of a designed system – is faciliated by designers who are oblivious to the politics of digital infrastructure or consider their design practice output to be apolitical. Despite traumatic events against users with increasing regularity, addressing the weaponisation of design is not yet a priority, and is still to be addressed by the design community.

---
![A pixel-art style illustration of a hand holding a cell phone. The screen displays a burning building with a cheerful “Happy New Year!” superimposed over the smoke and flames.](https://shiba.computer/essay/happy-new-year.png)

The lived experiences of digital platform users is at odds with how these systems are conceived and built. Weaponised design – a process that allows for harm of users within the defined bounds of a designed system – is facilitated by practitioners who are oblivious to the politics of digital infrastructure or consider their design practice output to be apolitical. Although users find themselves subject to traumatic events with increasing regularity, weaponised design is yet to be addressed by the multi-faceted field of interface and infrastructure design.

> When people fail to follow these bizarre, secret rules, and the machine does the wrong thing, its operators are blamed for not understanding the machine, for not following its rigid specifications. With everyday objects, the result is frustration. With complex devices and commercial and industrial processes, the resulting difficulties can lead to accidents, injuries, and even deaths. It is time to reverse the situation: to cast the blame upon the machines and their design. It is the machine and its design that are at fault. It is the duty of machines and those who design them to understand people. It is not our duty to understand the arbitrary, meaningless dictates of machines[1](https://shiba.computer/essay/on-weaponised-design#fn1-29532 "see footnote").
> 
> – Donald Norman, The Design of Everyday Things

User Experience Design has blossomed from a niche industry in the halls of Silicon Valley’s early startup darlings to a force that architects our digital lives. Today, user experience design is wholly responsible for modelling human expression and self identity, enabling interaction and participation online. Design shares as much in common with information security research as it does with behavioural science or aesthetics. By failing to criticise common design practices or form cooperative relationships with other technology fields, user experience designers are effectively refusing to recognise and respond to traumatic cases of its own work being used to harm the users it claims to serve.

Every major technology company has grappled with examples of their platform being leveraged to maliciously harm users whilst performing entirely within expected behaviour. This is _**weaponised design**_ – electronic systems whose designs either do not account for abusive application or whose user experiences directly empower attackers.

Examples of weaponised design are numerous and horrific: Using a smartphone, a technology reporter uncovers the home address of a friend by interacting with a new Snapchat feature that broadcasts her location on a map each time she posts[2](https://shiba.computer/essay/on-weaponised-design#fn2-29532 "see footnote"). After viewing her friend’s posts, the journalist spends a few moments pinpointing her apartment via Google Maps street view and asks her subject to confirm the address. Her friend is ‘creeped out’ as she realises that, wholly unexpectedly, her phone has violated her privacy and her trust by exposing her home address to all of her Snapchat followers without her consent. The experience makes international headlines and authorities issue another warning over the app’s potential for targeted misuse.

Meanwhile, in an effort to address targeted harassment of users, the design team at Twitter announces an adjustment that silences notifications sent to trolling victims when they are added to malicious Twitter lists[3](https://shiba.computer/essay/on-weaponised-design#fn3-29532 "see footnote"). The outcry is immediate; Removing list notifications allows harassers to compile and share lists of targets undetected. Victims are unaware they are shared targets and can’t fight back. The violation is so obvious the feature is reversed within hours[4](https://shiba.computer/essay/on-weaponised-design#fn4-29532 "see footnote"). Harassment across Twitter continues.

Elsewhere, two Facebook designers write[5](https://shiba.computer/essay/on-weaponised-design#fn5-29532 "see footnote") about the data-driven tweaks their team has made to Newsfeed. They conclude that the changes make Facebook meaningfully easier to use, engage with and navigate, but fail to discuss the ethical or sociological implications of their work[6](https://shiba.computer/essay/on-weaponised-design#fn6-29532 "see footnote"), which researchers consider significant[7](https://shiba.computer/essay/on-weaponised-design#fn7-29532 "see footnote"). The article proves popular with the design community. A month later, the US Government enters the company's Menlo Park offices with a warrant, looking for evidence of Facebook’s ability to exert major influence on voting behaviour through psychology research enabled by interface and systems design[8](https://shiba.computer/essay/on-weaponised-design#fn8-29532 "see footnote"). These examples happened in 2017 alone and are all dramatic scenarios of weaponised design.

The most common way design is weaponised is through _poorly considered tradeoffs_, where the ideal user is prioritised over empathetic threat modelling, resulting in features that are at high risk of exploitation. Alongside this year’s Snapchat example, another example of this famously occurred in 2014 when attackers accessed automatic camera roll cloud backups and leaked of intimate photos of mostly female victims from Apple servers[9](https://shiba.computer/essay/on-weaponised-design#fn9-29532 "see footnote"). In that case, the critical key focus is not of the actions of attackers, but rather one of informed consent. One can assume Apple’s designers took a puritan, anti-sex approach to the problem of centralised photo backup: Users don’t sext, and if they do, they wouldn’t object to intimate personal photos being synced to the cloud. As the company transferred millions of users into an opt-out automatic backup service, they failed to articulate the personal implications to their user base.

Design can also be weaponised through team _apathy_ or _inertia_, where user feedback is ignored or invalidated by an arrogant, culturally homogenous or inexperienced team designing a platform. This is a notable criticism of Twitter’s product team, whose perceived lack of design-led response is seen as a core factor for enabling targeted, serious harassment of women[10](https://shiba.computer/essay/on-weaponised-design#fn10-29532 "see footnote") by _#Gamergate_[11](https://shiba.computer/essay/on-weaponised-design#fn11-29532 "see footnote"), from at least 2014 to present day.

Finally, design can be directly weaponised by the design team itself. Examples of this include Facebook’s designers conducting secret and non-consensual experiments on voter behaviour in 2012–2016, and emotional states of users in 2012[12](https://shiba.computer/essay/on-weaponised-design#fn12-29532 "see footnote"), and Target, who in 2014 through surveillance ad tech and careful communications design, informed a father of his daughter’s unannounced pregnancy[13](https://shiba.computer/essay/on-weaponised-design#fn13-29532 "see footnote"). In these examples, designers collaborate with other teams within an organisation, facilitating problematic outcomes whose impact scale exponentially in correlation with the quality of the design input.

These three situations have one thing in common: They are the result of designers failing their users through designed systems that behave more or less as the user expected. This is a decade long misrepresentation of the relationship and responsibilities of designers to their users, a dangerous lack of professionalism, ethics and self-regulation, along with a lack of understanding of how multi-disciplinary design is leveraged to both exploit and harm the community. As appointed user advocates, design is yet to embrace the new tools and practices to continue working in an increasingly user-hostile digital world.

### Corrupting Design

In 1993, Apple Computer’s Human Computer Interface research team hired cognitive psychologist Donald Norman to work with the company’s early-stage interaction team. While they were not the first to recognise the need for human-centric approaches to interface design[14](https://shiba.computer/essay/on-weaponised-design#fn14-29532 "see footnote"), Norman’s role as a ‘User Experience Architect’ marked a subtle but fundamental shift in the practice behind the design of commodified personal computer systems. By drawing from mid-twentieth century concepts of product design and applying psychology and behavioural science to information architecture, the foundations for user-centric software design were laid, contributing to the company’s early successes in the West.

Today, user experience design no longer requires a sociological or behavioural science background, but these origins linger. The field encompasses everything from aesthetics, visual communication and branding to deep system and information architecture, all working in concert to define and anticipate the activities of a user through applied user research. As platforms became more commodified – especially through mobile touch mediums – UX designers have progressively become more reliant on existing work, creating a feedback loop that promotes playfulness, obviousness and assumed trust at the expense of user safety.

The focus on details and delight can be traced to manifestos like Steve Krug’s _Don’t Make Me Think_, which propose a dogmatic adherence to cognitive obviousness and celebrates frictionless interaction as the ultimate design accomplishment. Users should never have to question an interface. Instead, designers should anticipate and cater to their needs: _“It doesn’t matter how many times I have to click, as long as each click is a mindless, unambiguous choice.[15](https://shiba.computer/essay/on-weaponised-design#fn15-29532 "see footnote")”_

A “mindless, unambiguous choice” is not without cultural, social and political context. In _Universal UX Design: Building Multicultural User Experience_, Alberto Ferreira explores the futility of ‘universal design,’ where in a globalised world, the expression of design varies wildly. Amongst many cross-cultural examples, his most striking are the failings of Western designers as they adapt their work to users in China or Japan. These hyper-connected middle-class audiences share comparable statuses for wealth, connectivity and education. But the cultural and aesthetic variances between these three societies are pronounced. Designers who produce or adapt work for these populations without a sturdy conceptual framework repeatedly fail their users[16](https://shiba.computer/essay/on-weaponised-design#fn16-29532 "see footnote"). _‘Mindless and unambiguous’_ is only true for those who have both the cultural context to effortlessly decode an interface, and the confidence that their comprehension is solid. Not only is this dogma an unreasonable constraint, it also frequently fails.

![Four screenshots of Facebook's Year In Review app. Clockwise from top left: Eric Meyer's deceased daughter, an urn containing the remains of a Facebook user's parent, a house fire and a recently-deceased beloved pet dog. Each image in the Year in Review app is picked algorithmically and placed against a celebratory new-year themed design.](https://shiba.computer/essay/year-in-review.png)

Four screenshots of Facebook’s Year In Review app. Clockwise from top left: Eric Meyer’s deceased daughter, an urn containing the remains of a Facebook user’s parent, a house fire and a recently-deceased beloved pet dog. Each image in the Year in Review app is picked algorithmically and placed against a celebratory new-year themed design.

In 2014, Eric Meyer, an influential web developer, was shown a celebratory algorithmically- generated “Facebook Year in Review” featuring images of his daughter who had died from cancer in that same year. He wrote:

> Algorithms are essentially thoughtless. They model certain decision flows, but once you run them, no more thought occurs. To call a person “thoughtless” is usually considered a slight, or an outright insult; and yet, we unleash so many literally thoughtless processes on our users, on our lives, on ourselves. \[…\] In creating this Year in Review app, there wasn’t enough thought given to cases like mine or anyone who had a bad year. The design is for the ideal user, the happy, upbeat, good-life user. It doesn’t take other use cases into account[17](https://shiba.computer/essay/on-weaponised-design#fn17-29532 "see footnote").”

That the distinguished design team at Facebook chose to deploy an application capable of such an insensitive intrusion betrays a lack of diverse life experiences within its ranks, but this is somehow a tame example of the homogenous foundations of user experience design. At its extreme, Twitter is often singled out for its initial ideal user design, and through designing with such optimism, it has become optimised for abuse – _“a honeypot for assholes[18](https://shiba.computer/essay/on-weaponised-design#fn18-29532 "see footnote")”_. Now that the platform is an incumbent in the social media landscape, investor demand for user attention – measured as ‘platform stickiness’ – modifying the weaponised effects of homogenous ideal design has created business incentives to prevent the implementation of subsequent protections. In direct contradiction to public claims by the company’s leadership, former[19](https://shiba.computer/essay/on-weaponised-design#fn19-29532 "see footnote") and current[20](https://shiba.computer/essay/on-weaponised-design#fn20-29532 "see footnote") Twitter employees describe a organisation-wide disinterest in creating new models to moderate abuse on their platform or even deploy already-developed solutions.

As design has become commodified and weaponised by both platform operators and attackers, the response from designers has largely been to arrange chairs on the Titanic. As designer David Rudnick wrote in a series of tweets:

> Its not just that 20th \[century\] design laid the groundwork for the centralisation of global power, it actively belittled strategies for resistance. It sneered & recast radical voices, voices of colour, of poverty, as aesthetics of failure. Then told us to trust a system it helped rig.[21](https://shiba.computer/essay/on-weaponised-design#fn21-29532 "see footnote")”

Today’s designers continue to maintain the status quo without industry or external criticism or pressure. Whereas other industries have ethical review boards or independent investigative bodies, popular design is apathetic[22](https://shiba.computer/essay/on-weaponised-design#fn22-29532 "see footnote") to harm reduction or political discussion. Because of this, the industry is yet to fully examine its participation to the current techno-political climate. In 2017 – as wealthy democracies destabilise and technology is embraced in conflict-stricken societies – the ethical and empathetic failings of user experience design harms users with increasing regularity and intensity.

### Responding to a user-hostile world

Design is inherently political, but it is not inherently good. With few exceptions, the motivations of a design project are constrained by the encompassing platform or system first, and the experiences and values of its designers second. The result is designers working in a user hostile world, where even seemingly harmless platforms or features are exploited for state or interpersonal surveillance and violence.

As people living in societies, we cannot be separated from our political contexts. However, design practitioners research and implement systems based on a process of abstracting their audience through user stories. A user story is _“a very high-level definition of a requirement, containing just enough information so that the developers can produce a reasonable estimate of the effort to implement it[23](https://shiba.computer/essay/on-weaponised-design#fn23-29532 "see footnote").”_ In most cases, user are grouped through shared financial or biographical data, by their chosen devices, or by their technical or cognitive abilities.

When designing for the digital world, user stories ultimately determine what is or is not an acceptable area of human variation. The practice empowers designers and engineers to communicate via a common problem-focused language. But practicing design that views users through a politically-naive lens leaves practitioners blind to the potential weaponisation of their design. User-storied design abstracts an individual user from a person of lived experience to a collection of designer-defined generalisations. In this approach, their political and interpersonal experiences are also generalised or discarded, creating a shaky foundation that allows for assumptions to form from the biases of the design team. This is at odds with the personal lived experience of each user, and the complex interpersonal interactions that occur within a designed digital platform.

When a design transitions from theoretical to tangible, individual user problems and motivations become part of a larger interpersonal and highly political human network, affecting communities in ways that we do not yet fully understand. In _Infrastructural Games and Societal Play_, Eleanor Saitta writes of the rolling anticipated and unanticipated consequences of systems design: _“All intentionally-created systems have a set of things the designers consider part of the scope of what the system manages, but any nontrivial system has a broader set of impacts. Often, emergence takes the form of externalities — changes that impact people or domains beyond the designed scope of the system[24](https://shiba.computer/essay/on-weaponised-design#fn24-29532 "see footnote").”_ These are no doubt challenges in an empathetically designed system, but in the context of design homogeny, these problems cascade.

In a talk entitled _From User Focus to Participation Design_, Andie Nordgren advocates for how participatory design is a step to developing empathy for users:

> “If we can’t get beyond ourselves and our \[platforms\] – even if we are thinking about the users – it’s hard to transfer our focus to where we actually need to be when designing for participation which is with the people in relation to each other[25](https://shiba.computer/essay/on-weaponised-design#fn25-29532 "see footnote").”

Through inclusion, participatory design extends a design team’s focus beyond the hypothetical or ideal user, considering the interactions between users and other stakeholders over user stories. When implemented with the aim of engaging a diverse range of users during a project, participatory design becomes more political by forcing teams to address weaponised design opportunities during all stages of the process.

Beyond better design paradigms, designers must look beyond the field, toward practices that directly criticise or oppose their work. In particular, security research and user experience design have significant practice and goal overlap and this relationship is often antagonistic. Both fields primarily focus on the systems of wide-scale interactions between users and technology, but the goals of the two fields are diametrically opposed; design is to create the best possible experience for a user, security is to create the worst possible experience for an attacker. By focusing on the outcomes of the two fields, it’s clear that security research is a form of user experience design. Design should reciprocate, and become a form of security research.

At-risk users themselves are educating each other in operational security and threat modelling techniques. Robert Gehl describes how, increasingly, the threats faced by dark web communities mirror those experienced by the general public, acting as predictors for the future of the internet. In his essay, _Proactive Paranoia_, he writes:

> Proactive paranoia was forged in the toxic cultural contexts of increasingly militarized states, ubiquitous surveillance, global neoliberalism, and an all-out hustle for online money. In other words, OPSEC politics need not be limited to the dark web. In a world of arrests, scams, fines and fees, constant monitoring, and extreme caveat emptor, suspicion and paranoia are rational responses — not just in dark web markets, but in our daily lives[26](https://shiba.computer/essay/on-weaponised-design#fn26-29532 "see footnote").”

There is tremendous opportunity for designers and information security researchers to cooperatively apply operational security and threat modelling practices to their work. If adopting participation design enables greater focus on interpersonal interactions within a network, then threat modelling and operational security practices offer concrete foundations for addressing human-design socio-political threats within a platform.

### The political designer

It is critical that user experience design must begin to deconstruct the outcomes of our collective body of work, especially as tech becomes more embedded and less visible or more easily ignored. Saitta writes, _“All infrastructure is political; indeed, one might better say that all politics is infrastructural; we ignore it at our peril.”_ Developing new design practices helps to reduce cases of weaponisation through trade-offs in a system’s design, but practice alone is not enough.

Despite the many problematic elements of contemporary design, the community is somewhat capable of self-examination and cultural change. Counterintuitively, within user experience and its related fields, gender inequality seems less pronounced than the incredibly low bar set by other fields within the technology industries[27](https://shiba.computer/essay/on-weaponised-design#fn27-29532 "see footnote"). Self-reported research suggests that entry level roles are both well-paid and tend towards equality, with a smaller pay and ratio gap between men and women[28](https://shiba.computer/essay/on-weaponised-design#fn28-29532 "see footnote"). This is not to say that the industry is an inclusive field, and furthermore it is unclear whether these trends extend into senior or managerial positions.

The introduction of the Open Code of Conduct[29](https://shiba.computer/essay/on-weaponised-design#fn29-29532 "see footnote") is one example of industry self-reflection and regulation. Designed to hold the contributors of a project to account, the commitment of members to a minimum level of expected behaviour as defined in a code of conduct has its roots in organiser or maintainer response to incidents of harassment at conferences and in open source projects[30](https://shiba.computer/essay/on-weaponised-design#fn30-29532 "see footnote").

But addressing problematic internal culture of design teams is not enough. As an industry we must also confront the real-world socio-political outcomes of our practice. If we accept a code of conduct as necessary, we must also accept a code of outcomes as necessary. We must create ethical frameworks to evaluate our work at all stages, especially once it is alive in the world. Our lack of ongoing critical evaluation of our profession means that design continues to reinforce a harmful status quo, creating exploitable systems at the expense of societies.

User experience is more than just aesthetics and interfaces. It is a form of cooperative authorship and bound deeply to the infrastructure; each platform and its designers work together to represent a piece of an individual’s digital self and self expression within a wider online community. This is a responsibility on par with publishing literature that transports the reader so fully as to transform one’s self-understanding or producing cinema that ensconces the viewer so deeply that at the end there is a moment of pause to remember which understanding is reality. The online lives we collectively live are inherently mediated by technology, but how we experience this is also mediated by design.

Technology inherits the politics of its authors, but almost all technology can be harnessed in ways that transcend these frameworks. Even in its harmful and chaotic state, it is still possible to marvel at the opportunities afforded to us through design. But arrogant practice that produce brutally naive outcomes must be transcended to facilitate empathetic products and broader platforms. We must stop making decisions inside of a rose-coloured echo-chamber. As designers it is time to collectively address the political, social and human costs of weaponised design.

■ Berlin, Winter 2018.

_Commissioned by [Tactical Tech](https://tacticaltech.org/) and included in ‘[Our Data Our Selves](https://ourdataourselves.tacticaltech.org/projects/data-and-you/)’ a collection of essays on Data and Discrimination. Edited by Maya Ganesh & Arikia Millikan. Special thanks to Rose Regina Lawrence, Sema Karaman, Dalia Othman, Louis Center and Ignatius Gilfedder._

~

1.  Donald Norman, [The Design of Everyday Things](https://www.amazon.com/Design-Everyday-Things-Donald-Norman/dp/1452654123), Basic Books, Revised 2013. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr1-29532 "return to article")
    
2.  Dani Deahal, [Snapchat’s newest feature is also its biggest privacy threat](https://www.theverge.com/2017/6/23/15864552/snapchat-snap-map-privacy-threat), The Verge, 2017. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr2-29532 "return to article")
    
3.  Twitter first announced the change via [its own platform](https://twitter.com/TwitterSafety/status/831247282544599040). Responses from prominent critics are threaded contextually with the original tweet. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr3-29532 "return to article")
    
4.  Sarah Perez, [Twitter quickly kills a poorly thought out anti-abuse measure](https://techcrunch.com/2017/02/14/twitter-quickly-kills-a-poorly-thought-out-anti-abuse-measure/), Tech Crunch, 2017. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr4-29532 "return to article")
    
5.  Shali Nguyen & Ryan Freitas, [Evolving the Facebook News Feed to Serve You Better](https://medium.com/facebook-design/evolving-the-facebook-news-feed-to-serve-you-better-f844a5cb903d), Medium.com / Facebook Design, 2017. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr5-29532 "return to article")
    
6.  While not a direct criticism to the Newsfeed team’s work, Zeynep Tufekci’s [_Engineering the public: Big data, surveillance and computational politics_](http://firstmonday.org/article/view/4901/4097) (2014) was an early influential investigation of Facebook’s practices. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr6-29532 "return to article")
    
7.  Jason J. Jones, Robert M. Bond, Eytan Bakshy, Dean Eckles & James H. Fowler, [Social influence and political mobilization: Further evidence from a randomized experiment in the 2012 U.S. presidential election](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0173851) PLOS ONE, 2012. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr7-29532 "return to article")
    
8.  At time of writing, the US Justice Department had only just begun [its investigation](https://www.wsj.com/articles/facebook-gave-special-counsel-robert-mueller-more-details-on-russian-ad-buys-than-congress-1505514552) into electioneering via social media platforms and ad buys. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr8-29532 "return to article")
    
9.  [Relevant ‘celebgate’ context](https://en.wikipedia.org/wiki/ICloud_leaks_of_celebrity_photos), detailing the full extent of the phishing attack and resulting fallout. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr9-29532 "return to article")
    
10.  Matias, J. N., Johnson, A., Boesel, W. E., Keegan, B., Friedman, J., & DeTar, C., [Reporting, Reviewing, and Responding to Harassment on Twitter](https://womenactionmedia.org/cms/assets/uploads/2015/05/wam-twitter-abuse-report.pdf), Women, Action, and the Media, 2015. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr10-29532 "return to article")
    
11.  ‘#GamerGate’ is an extremely visible example of [gendered online violence](http://www.academia.edu/9790919/Sexism_in_the_Circuitry_Female_Participation_in_Male_Dominated_Popular_Computer_Culture). [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr11-29532 "return to article")
    
12.  Adam D. I. Kramer, Jamie E. Guillory and Jeffrey T. Hancock, [Experimental evidence of massive-scale emotional contagion through social networks](http://www.pnas.org/content/111/24/8788.full.html), Proceedings of the National Sciences of the United States of America, 2014. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr12-29532 "return to article")
    
13.  This is an anecdote in a 2012 New York Times piece, _[How Companies Learn Your Secrets](http://www.nytimes.com/2012/02/19/magazine/shopping-habits.html)_, and is an earlier publicised example of big data and corporate surveillance on the middle class. When initially published, this story went viral. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr13-29532 "return to article")
    
14.  There were several other key players responsible for pioneering graphical user interfaces and user experience, including Susan Kare, Alan Kay, Larry Tesler, Bob Taylor, and many others. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr14-29532 "return to article")
    
15.  Steve Krug, [Don’t Make Me Think](https://www.amazon.com/Dont-Make-Me-Think-Usability/dp/0321344758), New Riders Press, 2000. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr15-29532 "return to article")
    
16.  Alberto Ferreira, [Universal UX Design: Building Multicultural User Experience](https://books.google.de/books?id=pFaZBQAAQBAJ), Morgan Kaufmann, 2016. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr16-29532 "return to article")
    
17.  [Eric’s experience](http://meyerweb.com/eric/thoughts/2014/12/24/inadvertent-algorithmic-cruelty/) was not unique, nor was he the only person to write about his experience. Facebook’s _Year in Review_ has generated [annual](https://mashable.com/2016/12/08/2016-facebook-year-in-review-reactions/#u0xqZHwGysq3) [criticism](https://www.engadget.com/2017/12/06/facebook-year-in-review-2017/) since it first debuted. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr17-29532 "return to article")
    
18.  Charlie Warzel, [“A Honeypot For Assholes”: Inside Twitter’s 10-Year Failure To Stop Harassment](https://www.buzzfeed.com/charliewarzel/a-honeypot-for-assholes-inside-twitters-10-year-failure-to-s), Buzzfeed, 2016. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr18-29532 "return to article")
    
19.  [Criticism](https://twitter.com/saffree/status/913930925544583168) by @saffree, a former Twitter employee. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr19-29532 "return to article")
    
20.  Joshua Cohen (@heyjoshua) was a Twitter employee while [posting criticism](https://twitter.com/heyjoshua/status/914659305391116289) pf the company’s inaction against abuse. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr20-29532 "return to article")
    
21.  Quoted from a [Twitter thread](https://twitter.com/David_Rudnick/status/912026171168186368) by @David\_Rudnick. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr21-29532 "return to article")
    
22.  This [2016 collection](https://medium.com/swlh/mediums-best-design-writing-of-2016-68de5ed2b7d9) of popular design writing is an example of the industry’s fixation on more trivial issues as larger societal problems were beginning to converge. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr22-29532 "return to article")
    
23.  A [deeper introduction](http://www.agilemodeling.com/artifacts/userStory.htm) to user stories and the Agile software development methodology. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr23-29532 "return to article")
    
24.  Eleanor Saitta, [Infrastructural Games and Societal Play](https://dymaxion.org/essays/infrastructuralgames.html), Dymaxion.org, 2016. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr24-29532 "return to article")
    
25.  Andie Nordgren, [From User Focus To Participation Design](https://www.youtube.com/watch?v=tSmI4KO9968), Alibis for Interaction, 2014. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr25-29532 "return to article")
    
26.  Robert Gehl, [Proactive Paranoia](http://reallifemag.com/proactive-paranoia/), Real Life Magazine, 2017. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr26-29532 "return to article")
    
27.  Kate Crawford, [Artificial Intelligence’s White Guy Problem](https://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html), New York Times, 2016. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr27-29532 "return to article")
    
28.  The majority of employment and gender reporting is presented by industry bodies. In this case, the cited research was published by the [User Experience Professionals Association](https://uxpa.org/), a US organisation. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr28-29532 "return to article")
    
29.  For an introduction to Code of Conducts within the open source software community, see the [Open Source Code of Conduct guide](https://opensource.guide/code-of-conduct/). [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr29-29532 "return to article")
    
30.  In 2015, Github [adopted](https://github.com/blog/2039-adopting-the-open-code-of-conduct) Code of Conducts as an official feature and policy, influencing the further spread of the concept. [↩︎](https://shiba.computer/essay/on-weaponised-design#fnr30-29532 "return to article")
